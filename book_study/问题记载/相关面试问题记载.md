### 1. 定时器是怎么实现的？还有什么实现方式？

>> 答：   
>> * 理解定时器： 很多场景会使用到定时器，例如     
>> 1.使用TCP长连接时，客户端需要定时向服务端发送心跳请求。   
>> 2.财务系统每个月的月末定时生成对账单。  
>> 3.双11的0点，定时开启秒杀开关。   
>> * 一般定时任务的形式表现为：经过固定时间后触发、按固定频率周期性触发、在某个时刻触发。
定时器可以理解为这样一个数据结构：存储一系列的任务集合，并且Deadline越接近的任务，
拥有越高的执行优先级，在用户视角支持以下几种操作：NewTask:将新任务加入任务集合，
Cancel：取消某个任务，在任务调度视角还要支持：Run:执行一个到期的定时任务，判断一个
任务是否到期，基本会采用轮询的方式进行过，每隔一个时间片去检查最近的任务是否到期，并且在
NewTask和Cancel的行为发生后，任务调度策略也会出现调整。  
>> * 数据结构的选择包括：1. 双向有序链表，2. 堆， 3. 时间轮， 4.层级时间轮。   
>> * 常见的实现
>> 1. JDK Timer: 使用Timer实现任务调度的核心是Timer和TimerTask。其中Timer
负责设定TimerTask的起始和间隔执行时间。使用者只需要创建一个TimerTask的继承类，
实现自己的run方式，然后丢给Timer去执行即可。其中Timer::TaskQueue是使用数组
实现的一个简易的堆。其缺陷是a.只能被单线程调度，b.TimerTask中出现的异常会影响到
Timer的执行。  
>> 2. JDK ScheduledExecutorService： 解决了同一个定时器调度多个任务的阻塞问题，
并且任务异常不会中断ScheduledExecutorService。底层使用的数据结构为堆。
>> 3. JDK HashedWheelTimer。

>> [网络编程中定时器的实现](https://bingtaoli.github.io/2017/06/13/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%AD%E5%AE%9A%E6%97%B6%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0/)   
>> [参定时器的几种实现方式](https://www.cnkirito.moe/timer/)   
>> [Linux 实现定时器](https://liqiang.io/post/four-way-to-implement-linux-cron)    
>> [Linux C/C++定时器的实现原理和使用方法](https://blog.csdn.net/thisinnocence/article/details/81073117)     

### 2. [实现一个无锁队列(用原子操作)]([https://github.com/Apriluestc/2020/blob/master/doc/%E7%AE%97%E6%B3%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E7%AE%97%E6%B3%95.md#%E6%97%A0%E9%94%81%E9%98%9F%E5%88%97](https://github.com/Apriluestc/2020/blob/master/doc/算法数据结构/算法.md#无锁队列))

```cpp
bool compare_and_swap (int *addr, int oldval, int newval)
{
  if ( *addr != oldval ) {
      return false;
  }
  *addr = newval;
  return true;
}

EnQueue(x) {
    // 准备新加入的结点数据
    q = newrecord();
    q->value = x;
    q->next = nullptr;
    do {
        p = tail; // 取链表尾指针的快照
    } while (CAS(p->next, nullptr, q) != TRUE); // 如果没有把结点链上，再试
    CAS(tail, p, q); // 置尾结点
                
}

EnQueue(x) {
    q = newrecord();
    q->value = x;
    q->next = NULL;
    p = tail;
    oldp = p;
    do {
        // 如果T1线程在用CAS更新tail指针的之前，线程停掉或是挂掉了，那么其它线程就进入死循环了
        while (p->next != nullptr)
            p = p->next;
    } while (CAS(p.next, NULL, q) != true);
    CAS(tail, oldp, q); 
}

EnQueue(Q, data) //进队列改良版 v2 
{
    n = new node();
    n->value = data;
    n->next = NULL;
 
    while(TRUE) {
        //先取一下尾指针和尾指针的next
        tail = Q->tail;
        next = tail->next;
 
        //如果尾指针已经被移动了，则重新开始
        if ( tail != Q->tail ) continue;
 
        //如果尾指针的 next 不为NULL，则 fetch 全局尾指针到next
        if ( next != NULL ) {
            CAS(Q->tail, tail, next);
            continue;
        }
 
        //如果加入结点成功，则退出
        if ( CAS(tail->next, next, n) == TRUE ) break;
    }
    CAS(Q->tail, tail, n); //置尾结点
}

DeQueue() {
    do {
        p = head;
        if (p->next == nullptr) {
            return ERR_EMPTY_QUEUE;
        }
    }while (CAS(head, p, p->next) != true);
    return p->next->value;
}

DeQueue(Q) //出队列，改进版
{
    while(TRUE) {
        //取出头指针，尾指针，和第一个元素的指针
        head = Q->head;
        tail = Q->tail;
        next = head->next;
 
        // Q->head 指针已移动，重新取 head指针
        if ( head != Q->head ) continue;
         
        // 如果是空队列
        if ( head == tail && next == NULL ) {
            return ERR_EMPTY_QUEUE;
        }
         
        //如果 tail 指针落后了
        if ( head == tail && next == NULL ) {
            CAS(Q->tail, tail, next);
            continue;
        }
 
        //移动 head 指针成功后，取出数据
        if ( CAS( Q->head, head, next) == TRUE){
            value = next->value;
            break;
        }
    }
    free(head); //释放老的dummy结点
    return value;
}
```

参考：[无锁队列的实现](https://coolshell.cn/articles/8239.html)

### 3. 网络库的io模型是怎么样的，为什么这个io模型是高性能的？

>> UNP中总结的IO模型有5种之多：`阻塞IO，非阻塞IO，IO复用，信号驱动IO，异步IO`。前四种都属于同步IO。阻塞IO不必说了。非阻塞IO ，IO请求时加上O_NONBLOCK一类的标志位，立刻返回，IO没有就绪会返回错误，需要请求进程主动轮询不断发IO请求直到返回正确。IO复用同非阻塞IO本质一样，不过利用了新的select系统调用，由内核来负责本来是请求进程该做的轮询操作。看似比非阻塞IO还多了一个系统调用开销，不过因为可以支持多路IO，才算提高了效率。信号驱动IO，调用sigaltion系统调用，当内核中IO数据就绪时以SIGIO信号通知请求进程，请求进程再把数据从内核读入到用户空间，这一步是阻塞的。
   异步IO，如定义所说，不会因为IO操作阻塞，IO操作全部完成才通知请求进程。

>> `Reactor[one loop per thread: non-blocking + IO multiplexing]`模型。muduo采用的是Reactors in thread有一个main Reactor负责accept(2)连接，然后把连接挂在某个sub Reactor中(muduo中采用的是round-robin的方式来选择sub Reactor)，这样该连接的所有操作都在那个sub Reactor所处的线程中完成。多个连接可能被分到多个线程中，以充分利用CPU。

>> muduo采用的是固定大小的Reactor pool，池子的大小通常根据CPU数目确定。也就是说线程数固定，这样程序的总体处理能力不会随连接数增加而下降。另外一个连接完全由一个线程管理，那么请求的顺序性有保证，突发请求也不会占满8个核(如果需要优化突发请求，可以考虑Reactors + thread pool)。这种方案把IO分派给多个线程，防止出现一个Reactor的处理能力饱和。

![各种IO模型的比较](https://github.com/834810071/muduo_study/blob/master/book_study/0_1280551552NVgW.jpg "各种IO模型的比较")

>> [5种网络IO模型（有图，很清楚）](https://www.cnblogs.com/findumars/p/6361627.html)   
>> [五种网络io模型](https://blog.csdn.net/WuLex/article/details/80615699)
>> [muduo网络库：Reactor模型的介绍](https://blog.csdn.net/weixin_43819197/article/details/92828590)

### 4. muduo的多线程体现在什么地方？

>> muduo是基于one loop per thread模型。字面意思上讲就是每个线程里有个loop，即消息循环。服务器必定有一个监听的socket和1到N个连接的socket，每个socket也必定有网络事件。我们可以启动设定数量的线程，让这些线程来承担网络事件。

>> 每个进程默认都会启动一个线程，即这个线程不需要我们手动创建，称之为主线程。一般地我们让主线程来承担监听socket的网络事件，然后等待新的连接。至于新连接的socket的事件要不要在主线程中处理，这个得看我们启动其他线程即工作线程的数量。如果启动了工作线程，那么新连接的socket的网络事件一定是在工作线程中处理的。

>> 每个线程的事件处理都是在一个EventLoop的while循环中，而每个EventLoop都有一个多路事件复用解析器epoller。循环的主体部分是等待epoll事件触发，从而处理事件。主线程EventLoop的epoller会监听socket可读事件，而工作线程一开始什么都没有添加，因为还没有连接产生。在没有事件触发之前，epoller都是阻塞的。导致线程被挂起。

>> 当有连接到来时，挂起的主线程恢复，会执行新连接的回调函数。在该函数中，会从线程池中取得一个线程来接管新的socket的处理。

>> muduo中多线程主要是`EventLoopThread(IO线程类)`、`EventLoopThreadPool(IO线程池类)`，IO线程池的功能是开启若干个IO线程，并让这些IO线程处于事件循环的状态。

>> `muduo` 采用`one loop per thread`的设计思想，即每个线程运行一个循环，这里的循环就是事件驱动循环`EventLoop`。所以，`EventLoop`对象的loop函数，包括间接引发的`Poller`的`poll`函数，`Channel`的`handleEvent`函数，以及`TcpConnection`的`handle*`函数都是在一个线程内完成的。而在整个`muduo`体系中，有着多个这样的`EventLoop`，每个`EventLoop`都执行着`loop,poll,handleEvent,handle*`这些函数。这种设计模型也被称为`Reactor + 线程池`。

>> 控制这些`EventLoop`的，保存着事件驱动循环线程池的，就是TcpServer类。顾名思义，服务器类，用来创建一个高并发的服务器，内部便有一个线程池，线程池中有大量的线程，每个线程运行着一个事件驱动循环，即`one loop per thread`。

>> 另外，`TcpServer`本身也是一个线程(主线程)，也运行着一个`EventLoop`，
这个事件驱动循环仅仅用来监控客户端的连接请求，即`Accetpor`对象的`Channel`的可读事件。
通常，如果用户添加定时器任务，也会由这个EventLoop监听。但是`TcpServer`的这个`EventLoop`不在线程池中，这一点要区分开，线程池中的线程只用来运行负责监控`TcpConnection`的`EventLoop`的。

>> [muduo 源码分析](https://blog.csdn.net/zxm342698145/article/category/9277603)    
>> [muduo学习笔记](https://www.cnblogs.com/ailumiyana/category/1341869.html)    
>> [muduo源码](https://blog.csdn.net/wk_bjut_edu_cn/category_7764745.html)    
>> [muduo网络库源码分析](https://blog.csdn.net/sinat_35261315/category_9270538.html)         
>> [muduo多线程异步日志分析 base还没有看](https://blog.csdn.net/puliao4167/article/details/89884107)

### 5. muduo的主线程accept的fd如何分发给其他线程？

​	TcpServer每次新建一个TcpConnection就会调用getNextLoop()来取得EventLoop，采用round-robin算法来选取pool中的EventLoop,不允许TcpConnection在运行中更换EventLoop。

### [7.muduo如何限制连接的数量？](http://www.cppblog.com/Solstice/archive/2011/04/27/145102.html)

* 解决文件描述符达到上限

  ​	准备一个空闲文件描述符。遇到这种情况，先关闭空闲文件，获得一个文件描述符的名额，再accept拿到新的socket连接的描述符，随后立刻close它，这样就优雅的断开了客户端连接，最后重新打开一个空闲文件。

* 增加一个 int 成员，表示当前的活动连接数。然后，在 EchoServer::onConnection() 中判断当前活动连接数，如果超过最大允许数，则踢掉连接。

### [8.muduo如何设计buffer？](https://www.cnblogs.com/solstice/archive/2011/04/17/2018801.html)

Muduo Buffer 的设计要点：

- 对外表现为一块连续的内存(char*, len)，以方便客户代码的编写。
- 其 size() 可以自动增长，以适应不同大小的消息。它不是一个 fixed size array (即 char buf[8192])。
- 内部以 vector of char 来保存数据，并提供相应的访问函数。

Buffer 其实像是一个 queue，从末尾写入数据，从头部读出数据。

TcpConnection 会有两个 Buffer 成员，input buffer 与 output buffer。

- input buffer，TcpConnection 会从 socket 读取数据，然后写入 input buffer（其实这一步是用 Buffer::readFd() 完成的）；客户代码从 input buffer 读取数据。
- output buffer，客户代码会把数据写入 output buffer（其实这一步是用 TcpConnection::send() 完成的）；TcpConnection 从 output buffer 读取数据并写入 socket。

其实，input 和 output 是针对客户代码而言，客户代码从 input 读，往 output 写。TcpConnection 的读写正好相反。

具体做法是，在栈上准备一个 65536 字节的 stackbuf，然后利用 readv() 来读取数据，iovec 有两块，第一块指向 muduo Buffer 中的 writable 字节，另一块指向栈上的 stackbuf。这样如果读入的数据不多，那么全部都读到 Buffer 中去了；如果长度超过 Buffer 的 writable 字节数，就会读到栈上的 stackbuf 里，然后程序再把 stackbuf 里的数据 append 到 Buffer 中。

![buffer0](https://images.cnblogs.com/cnblogs_com/Solstice/201104/201104171223592850.png)

​	两个 indices 把 vector 的内容分为三块：prependable、readable、writable，各块的大小是（**公式一**）：

* prependable = readIndex

* readable = writeIndex - readIndex

* writable = size() - writeIndex（prependable 的作用留到后面讨论。）

readIndex 和 writeIndex 满足以下不变式(invariant):

* 0 ≤ readIndex ≤ writeIndex ≤ data.size()

### [9.muduo的定时器是如何设计的？](http://www.cppblog.com/Solstice/archive/2014/08/21/139769.html)

>> 答:   
>>
>> * muduo的定时器功能由三个class实现，`TimerId`、`Timer`、`TimerQueue`，用户只能看到第一个class，另外两个都是内部实现细节。其中`TimerQueue`最重要的接口包括`addTimer()`添加定时任务和`cancel()`取消定时任务。  

>> * muduo把定时器交给内核管理，利用linux内核提供的`timerfd_create`接口创建定时器。     

>> * muduo中定义了`TimerQueue`类作为定时器的封装。初始化时候，通过linux的API创建`timerfd`。定义一个timers来存储timer,这个timers的存储结构是std::Set，元素是std::pair<Timerstamp, Timer*>，添加定时器则往timers添加，muduo使用的是set存储定时器，set是默认排序的，最早超时的会在最前面，即set.begin()为最早超时的定时器。   

>> * 获取超时任务是通过`getExpired()`函数实现的，其具体实现是根据传入的参数设置一个哨兵值，然后调用set::lower_bound()返回第一个未到期的Timer的迭代器，
之后在timers_中删除到期的任务，并返回到期的任务。muduo使用linux的API创建定时触发可读的timerfd_,fd触发时调用handleRead函数，
该函数调用getExpired获取超时定时器，并逐一执行。  

>> * cancel()通过给每个Timer对象添加一个全局递增序列号，区分地址相同的先后两个Timer对象。增加activeTimers_成员变量，保存的是目前有效的Timer指针，并满足`timers_.size() == activeTimers_.size()`，元素是pair<Timer*, int64_t>。并且利用`callingExpiredTimers_`和`cancelingTimers_`防止“自注销”现象，即定时器回调中注销当前定时器。 

### [10. 如何安全的关闭tcp连接，能不能直接close，如何直接close会发生什么？](https://blog.csdn.net/Solstice/article/details/6208634)

​	 调用TcpConnection::shutdown主动关闭时，muduo 把“主动关闭连接”这件事情分成两步来做，如果要主动关闭连接，它会先关本地“写”端，等对方关闭之后，再关本地“读”端。

​	“如果被动关闭连接，muduo 的行为如何？” 提示：muduo 在 read() 返回 0 的时候会回调 connection callback，这样客户代码就知道对方断开连接了。

​	直接关闭会影响收发数据的完整性。

### 11. muduo是如何 线程安全的对 对象的生命周期进行管理？

### 12. 介绍一下这个项目(几乎是必问的)

​	是一个基于Reactor模式的c++网络库，其中实现了定时器，Buffer缓存，多线程TcpServer等模块，主线程负责accept请求，并通过轮询方式分配给其他IO线程。

### 15.eventfd是什么？有什么好处？

* eventfd是linux的一个系统调用，为事件通知创建文件描述符，eventfd()创建一个“eventfd对象”，这个对象能被用户空间应用用作一个**事件等待/响应机制**，靠内核去响应用户空间应用事。
* 实现唤醒，让IO线程从IO multiplexing阻塞调用中返回，更高效地唤醒。

### [16.双缓冲区异步日志是什么？为什么要这样做？对这个日志系统有没有进行压力测试？](https://blog.csdn.net/daaikuaichuan/article/details/86500108)

* 日志库采用的是双缓冲技术，基本思路是准备两块缓冲：A与B，前端负责往buffer A中填数据（日志消息），后端负责将buffer B中的数据写入文件。当buffer A写满之后，交换A与B，让后端将buffer A中的数据写入文件，而前端负责往buffer B中填入新的日志文件。如此往复。用两个buffer的好处是在新建日志消息的时候不必等待磁盘文件操作，也避免每条消息都触发（唤醒）了后端日志线程。换言之，前端不是将一条条消息分别传送给后端，而是将多个日志消息拼成一个大的buffer传送给后端，相当于批处理，减少了线程唤醒的频率，降低了开销。另外，为了及时将消息写入文件，即使前端的buffer A未写满，日志库也会每三秒执行一次上述交换写入操作。
* ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190116164903956.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhYWlrdWFpY2h1YW4=,size_16,color_FFFFFF,t_70)
* 多个线程可以并发写日志，两个线程日志消息不会出现交织，并且满足高性能，不存在抢锁的现象。网络IO线程或业务线程中直接往磁盘写数据的话，写操作偶尔可能会阻塞长达数秒。导致请求超时或者耽误发送心跳消息，引起误报死锁，或者引发自动faileover等。
* 什么时候切换写到另一个日志文件？前一个buffer已经写满了，则交换两个buffer（写满的buffer置空）。

* 日志串写入过多，日志线程来不及消费，怎么办？直接丢掉多余的日志buffer，腾出内存，防止引起程序故障。

* 什么时候唤醒日志线程从Buffer中取数据？其一是超时，其二是前端写满了一个或者多个buffer。

参考: [muduo的日志库分析四之AsyncLogging类](https://www.twblogs.net/a/5b8dcefa2b7177188340bbb4/zh-cn)

### 17.什么是优雅关闭连接？(就是read()到0，要透明的传递这个行为而不是直接暴力close())

​	现在TcpServer中移除当前连接，然后在TcpConnectin::connectionDestroyed()中调用connectionCallback_通知关闭，最后在loop_中移除当前channel_。

### [18.epoll的边沿触发和水平触发有什么区别？(epoll的源码并不长，从源码的角度回答比较好)](https://www.cnblogs.com/charlesblc/p/6242479.html)

​	**水平触发(level-trggered)**

* 只要文件描述符关联的读内核缓冲区非空，有数据可以读取，就一直发出可读信号进行通知，

* 当文件描述符关联的内核写缓冲区不满，有空间可以写入，就一直发出可写信号进行通知

**LT模式支持阻塞和非阻塞两种方式。epoll默认的模式是LT。**

​	**边缘触发(edge-triggered)**

* 当文件描述符关联的读内核缓冲区由空转化为非空的时候，则发出可读信号进行通知，

* 当文件描述符关联的内核写缓冲区由满转化为不满的时候，则发出可写信号进行通知

*  两者的区别在哪里呢？水平触发是只要读缓冲区有数据，就会一直触发可读信号，而边缘触发仅仅在空变为非空的时候通知一次。

  * 执行epoll_create时，创建了**红黑树和就绪链表**，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后**向内核注册回调函数**，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。

  ![img](https://images2015.cnblogs.com/blog/899685/201701/899685-20170102144714222-318866749.png)

* LT, ET这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是**LT模式的句柄**了），**并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了**。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回这个句柄。（从上面这段，可以看出，LT还有个**回放的过程，低效了**）

### [19.epoll为什么高效，相比select和poll](https://www.cnblogs.com/charlesblc/p/6242479.html)

* 这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。

### 20.HTTP报文都有哪些字段？

**HTTP请求报文**

一个HTTP请求报文由请求行（request line）、请求头部（header）、空行和请求数据4个部分组成，下图给出了请求报文的一般格式。

![img](https://pic002.cnblogs.com/images/2012/426620/2012072810301161.png)

or

＜request-line＞

＜headers＞

＜blank line＞

[＜request-body＞]

**1.请求头**

请求行由请求方法字段、URL字段和HTTP协议版本字段3个字段组成，它们用空格分隔。例如，GET /index.html HTTP/1.1。

**2.请求头部**

请求头部由关键字/值对组成，每行一对，关键字和值用英文冒号“:”分隔。请求头部通知服务器有关于客户端请求的信息，典型的请求头有：

User-Agent：产生请求的浏览器类型。

Accept：客户端可识别的内容类型列表。

Host：请求的主机名，允许多个域名同处一个IP地址，即虚拟主机。

**3.空行**

最后一个请求头之后是一个空行，发送回车符和换行符，通知服务器以下不再有请求头。

**4.请求数据**

请求数据不在GET方法中使用，而是在POST方法中使用。POST方法适用于需要客户填写表单的场合。与请求数据相关的最常使用的请求头是Content-Type和Content-Length。

**HTTP报文**

HTTP响应也由三个部分组成，分别是：状态行、消息报头、响应正文。

如下所示，HTTP响应的格式与请求的格式十分类似：

＜status-line＞

＜headers＞

＜blank line＞

[＜response-body＞]

![img](https://images0.cnblogs.com/blog2015/776887/201507/241034588189239.png)

参考：[HTTP请求报文和HTTP响应报文](https://www.cnblogs.com/biyeymyhjob/archive/2012/07/28/2612910.html)

### 21.假如服务器要升级，又不想让用户感觉到服务器升级了，该怎么做？(其实就是不间断的提供服务，参考nginx的平滑升级)

​	**平滑升级的本质就是 listener fd 的迁移**

​	**Nginx 的平滑升级是通过 `fork` + `execve` 这种经典的处理方式来实现的。**准备升级时，Old Master 进程收到信号然后 `fork` 出一个子进程，注意此时这个子进程运行的依然是老的镜像文件。紧接着这个子进程会通过 `execve` 调用执行新的二进制文件来替换掉自己，成为 New Master。

​	 Nginx 在 `execve` 的时候压根就没有重新 `bind` + `listen`，而是直接把 listener fd 添加到 `epoll` 的事件表。因为这个 New Master 本来就是从 Old Master 继承而来，自然就继承了 Old Master 的 listener fd。

​	**环境变量**。`execve` 在执行的时候可以传入环境变量。实际上 Old Master 在 `fork` 之前会将所有 listener fd 添加到 `NGINX` 环境变量：

​	Nginx 在启动的时候，会解析 `NGINX` 环境变量，一旦检测到是继承而来的 socket，那就说明已经打开了，不会再继续 `bind` + `listen` 了：

​	参考[Nginx Internals](https://www.slideshare.net/joshzhu/nginx-internals)

### 22.有没有实现内存池？

### 23.一个请求到来具体的处理过程是怎样的？

### 24.线程的唤醒还有哪些方式？

* eventfd
* 信号管道
* 条件变量

### [25.怎么检查内存泄漏的？](https://www.cnblogs.com/skynet/archive/2011/02/20/1959162.html)

* 进行静态分析，如[mtrace](https://en.wikipedia.org/wiki/Mtrace)
* 使用相关命令，ps、top
  * ps  -aux | grep -E 'progream1|PID'

* 进行动态分析，如memwatch和valgrind工具

参考: [在Linux下检查内存泄露](https://blog.csdn.net/weixin_36343850/article/details/77856051)

### 26.用到了哪些智能指针和[RAII机制](https://www.jianshu.com/p/b7ffe79498be)，几种锁的区别是什么

* shared_ptr、unique_ptr、scope_ptr、weak_ptr

  * shared_ptr: TcpClient::Connector

  * unique_ptr: AsyncLogging::BufferVector

  * scope_ptr: EventLoop::poller   TcpConnection::socket_, channe_

    * scope_ptr是一个很类似auto_ptr的智能指针，它包装了new操作符在堆上分配的动态对象，能够保证动态创建的对象在任何时候都可以被正确地删除。 

      但scope_ptr的所有权更加严格，不能转让，一旦scope_ptr获得了对象的管理权，你就无法再从它那里取回来.

  * weak_ptr: 

### 27.任务队列是怎么实现的，除了加锁还有什么方式？

### 28.如何解决死锁？

### 29.怎么进行压测的？

### 30.为什么要用非阻塞io？

### [32. Reactor模式是什么？](https://www.cnblogs.com/doit8791/p/7461479.html)

​	Reactor模式首先是**事件驱动的，有一个或多个并发输入源，有一个Service Handler，有多个Request Handlers**

![img](http://www.blogjava.net/images/blogjava_net/dlevin/Reactor_Simple.png)

![img](https://images2017.cnblogs.com/blog/150046/201709/150046-20170901082719280-29887948.png)

![img](https://images2017.cnblogs.com/blog/150046/201709/150046-20170901082834187-1581301551.png)

![img](https://images0.cnblogs.com/blog2015/434101/201503/112151380898648.jpg)